//! Google GenAI (Gemini) provider adapter.
//!
//! This module implements the integration with Google's Generative Language API,
//! mapping Codex's internal `Prompt` structures to Google's request format and
//! parsing Google's streaming SSE responses back into Codex's `ResponseEvent` stream.
//!
//! API Reference: https://ai.google.dev/api/rest/v1beta/models/generateContent

use std::collections::HashMap;
use std::time::Duration;

use crate::ModelProviderInfo;
use crate::client_common::Prompt;
use crate::client_common::ResponseEvent;
use crate::client_common::ResponseStream;
use crate::client_common::tools::ToolSpec;
use crate::default_client::CodexHttpClient;
use crate::error::CodexErr;
use crate::error::ConnectionFailedError;
use crate::error::ResponseStreamFailed;
use crate::error::Result;
use crate::error::UnexpectedResponseError;
use crate::model_family::ModelFamily;
use crate::util::backoff;
use bytes::Bytes;
use codex_otel::otel_event_manager::OtelEventManager;
use codex_protocol::models::ContentItem;
use codex_protocol::models::FunctionCallOutputContentItem;
use codex_protocol::models::LocalShellAction;
use codex_protocol::models::ResponseItem;
use codex_protocol::protocol::SessionSource;
use eventsource_stream::Eventsource;
use futures::Stream;
use futures::StreamExt;
use futures::TryStreamExt;
use reqwest::StatusCode;
use serde::Deserialize;
use serde::Serialize;
use serde_json::Value as JsonValue;
use serde_json::json;
use tokio::sync::mpsc;
use tokio::time::timeout;
use tracing::debug;
use tracing::trace;

// ============================================================================
// JSON Schema Processing for Google GenAI Compatibility
// ============================================================================

/// Process a JSON schema to make it compatible with Google GenAI API.
/// Removes unsupported fields like `additionalProperties`.
fn process_json_schema_for_google_genai(schema: &JsonValue) -> JsonValue {
    match schema {
        JsonValue::Object(map) => {
            let mut processed = serde_json::Map::new();

            for (key, value) in map {
                // Skip additionalProperties as Google GenAI doesn't support it
                if key == "additionalProperties" {
                    continue;
                }

                // Recursively process all object values to catch nested additionalProperties
                processed.insert(key.clone(), process_json_schema_for_google_genai(value));
            }

            JsonValue::Object(processed)
        }
        JsonValue::Array(arr) => {
            let processed_arr: Vec<JsonValue> = arr.iter()
                .map(|item| process_json_schema_for_google_genai(item))
                .collect();
            JsonValue::Array(processed_arr)
        }
        _ => schema.clone(),
    }
}

// ============================================================================
// Type Definitions for Google GenAI API
// ============================================================================

/// A part of a message content in Google GenAI API.
/// Only one field should be set per Part instance.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiPart {
    /// Plain text content
    #[serde(skip_serializing_if = "Option::is_none")]
    pub text: Option<String>,

    /// Function call from the model
    #[serde(skip_serializing_if = "Option::is_none")]
    pub function_call: Option<GoogleGenAiFunctionCall>,

    /// Function response from tool execution
    #[serde(skip_serializing_if = "Option::is_none")]
    pub function_response: Option<GoogleGenAiFunctionResponse>,
}

/// A function call generated by the model.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiFunctionCall {
    /// The name of the function to call
    pub name: String,

    /// The arguments to pass to the function as a JSON object
    #[serde(skip_serializing_if = "Option::is_none")]
    pub args: Option<JsonValue>,
}

/// A function response containing the result of a function call.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiFunctionResponse {
    /// The name of the function that was called
    pub name: String,

    /// The response from the function as a JSON object
    pub response: JsonValue,
}

/// A content object containing a role and parts.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiContent {
    /// The role of the content producer (user or model)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,

    /// The parts that make up this content
    pub parts: Vec<GoogleGenAiPart>,
}

/// Tool definition for function calling.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiTool {
    /// Function declarations available to the model
    pub function_declarations: Vec<GoogleGenAiFunctionDeclaration>,
}

/// A function declaration describing a callable function.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiFunctionDeclaration {
    /// The name of the function
    pub name: String,

    /// A description of what the function does
    pub description: String,

    /// The parameters the function accepts, described as a JSON Schema object
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parameters: Option<JsonValue>,
}

/// Request payload for Google GenAI streaming content generation.
#[derive(Debug, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiRequest {
    /// The conversation contents
    pub contents: Vec<GoogleGenAiContent>,

    /// Optional system instructions for the model
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_instruction: Option<GoogleGenAiContent>,

    /// Optional tools available to the model
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<GoogleGenAiTool>>,

    /// Optional tool configuration
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_config: Option<GoogleGenAiToolConfig>,

    /// Optional generation configuration
    #[serde(skip_serializing_if = "Option::is_none")]
    pub generation_config: Option<GoogleGenAiGenerationConfig>,
}

/// Configuration parameters for content generation.
#[derive(Debug, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiGenerationConfig {
    /// Controls randomness in generation
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,

    /// Controls diversity via nucleus sampling
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,

    /// Maximum number of tokens to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_output_tokens: Option<i32>,

    /// Stop sequences for generation
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop_sequences: Option<Vec<String>>,
}

/// Configuration for tool usage.
#[derive(Debug, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiToolConfig {
    /// Function calling configuration
    pub function_calling_config: GoogleGenAiFunctionCallingConfig,
}

/// Configuration for function calling behavior.
#[derive(Debug, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiFunctionCallingConfig {
    /// The mode for function calling (AUTO, ANY, NONE, etc.)
    pub mode: String,

    /// Optional list of allowed function names
    #[serde(skip_serializing_if = "Option::is_none")]
    pub allowed_function_names: Option<Vec<String>>,
}

// ============================================================================
// Response Types
// ============================================================================

/// A streaming response chunk from Google GenAI.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiStreamChunk {
    /// The candidate responses
    #[serde(default)]
    pub candidates: Vec<GoogleGenAiCandidate>,

    /// Token usage information
    #[serde(skip_serializing_if = "Option::is_none")]
    pub usage_metadata: Option<GoogleGenAiUsageMetadata>,

    /// Model version used
    #[serde(skip_serializing_if = "Option::is_none")]
    pub model_version: Option<String>,
}

/// A candidate response from the model.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiCandidate {
    /// The content of this candidate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content: Option<GoogleGenAiContent>,

    /// Why the model stopped generating (optional in streaming)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub finish_reason: Option<String>,

    /// The index of this candidate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub index: Option<i32>,
}

/// Token usage metadata.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GoogleGenAiUsageMetadata {
    /// Number of tokens in the prompt
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prompt_token_count: Option<i32>,

    /// Number of tokens in the generated candidates
    #[serde(skip_serializing_if = "Option::is_none")]
    pub candidates_token_count: Option<i32>,

    /// Total token count
    #[serde(skip_serializing_if = "Option::is_none")]
    pub total_token_count: Option<i32>,
}

// ============================================================================
// Helper Functions
// ============================================================================

/// Constructs the full URL for Google GenAI streaming endpoint.
/// Replaces the {model} placeholder with the actual model name.
pub(crate) fn get_google_genai_streaming_url(provider: &ModelProviderInfo, model: &str) -> String {
    let base_url = provider.get_full_url(&None);
    base_url.replace("{model}", model)
}

// ============================================================================
// Request Mapping
// ============================================================================

/// Builds a Google GenAI request from a Codex Prompt.
///
/// Maps Codex's internal prompt representation to Google's API format:
/// - System instructions → generationConfig.systemInstruction field
/// - Conversation history → contents array with role + parts
/// - Tools → tools array with function declarations
pub(crate) fn build_google_genai_request(
    prompt: &Prompt,
    model_family: &ModelFamily,
) -> Result<GoogleGenAiRequest> {
    // Get system instructions
    let full_instructions = prompt.get_full_instructions(model_family);
    let system_instruction = if !full_instructions.is_empty() {
        Some(GoogleGenAiContent {
            role: Some("user".to_string()),
            parts: vec![GoogleGenAiPart {
                text: Some(full_instructions.to_string()),
                function_call: None,
                function_response: None,
            }],
        })
    } else {
        None
    };

    // Convert conversation history to Google contents format
    let mut contents: Vec<GoogleGenAiContent> = Vec::new();
    let mut function_call_names: HashMap<String, String> = HashMap::new();
    let input = prompt.get_formatted_input();

    for item in &input {
        match item {
            ResponseItem::Message { role, content, .. } => {
                // Collect all text from content items
                let mut text = String::new();
                for c in content {
                    match c {
                        ContentItem::InputText { text: t }
                        | ContentItem::OutputText { text: t } => {
                            text.push_str(t);
                        }
                        ContentItem::InputImage { .. } => {
                            // Google GenAI supports images, but for initial implementation
                            // we'll focus on text only as per the issue scope
                            // TODO: Add image support in future iteration
                        }
                    }
                }

                if !text.is_empty() {
                    // Map Codex roles to Google roles
                    let google_role = match role.as_str() {
                        "user" => "user",
                        "assistant" => "model", // Google uses "model" instead of "assistant"
                        _ => "user",            // Default to user for safety
                    };

                    contents.push(GoogleGenAiContent {
                        role: Some(google_role.to_string()),
                        parts: vec![GoogleGenAiPart {
                            text: Some(text),
                            function_call: None,
                            function_response: None,
                        }],
                    });
                }
            }
            ResponseItem::FunctionCall {
                name,
                arguments,
                call_id,
                ..
            } => {
                // Parse arguments string to JSON
                let args: JsonValue = serde_json::from_str(arguments).unwrap_or_else(|_| json!({}));
                function_call_names.insert(call_id.clone(), name.clone());

                contents.push(GoogleGenAiContent {
                    role: Some("model".to_string()),
                    parts: vec![GoogleGenAiPart {
                        text: None,
                        function_call: Some(GoogleGenAiFunctionCall {
                            name: name.clone(),
                            args: Some(args),
                        }),
                        function_response: None,
                    }],
                });
            }
            ResponseItem::FunctionCallOutput { call_id, output } => {
                // For Google, we need to use the function name from the call
                // We'll use "function_response" as a generic name if not available
                // The content is the output
                let response_json = if let Some(items) = &output.content_items {
                    // If we have structured content items, serialize them
                    let text_parts: Vec<String> = items
                        .iter()
                        .filter_map(|it| match it {
                            FunctionCallOutputContentItem::InputText { text } => Some(text.clone()),
                            _ => None,
                        })
                        .collect();
                    json!({ "output": text_parts.join("\n") })
                } else {
                    json!({ "output": output.content })
                };

                let function_name = function_call_names
                    .get(call_id)
                    .cloned()
                    .unwrap_or_else(|| "function".to_string());

                contents.push(GoogleGenAiContent {
                    role: Some("user".to_string()), // Function responses come from user
                    parts: vec![GoogleGenAiPart {
                        text: None,
                        function_call: None,
                        function_response: Some(GoogleGenAiFunctionResponse {
                            name: function_name,
                            response: response_json,
                        }),
                    }],
                });
            }
            ResponseItem::LocalShellCall {
                id,
                call_id,
                action,
                status: _,
            } => {
                let call_identifier = call_id.as_ref().or(id.as_ref());
                if let Some(identifier) = call_identifier {
                    function_call_names
                        .entry(identifier.clone())
                        .or_insert_with(|| "local_shell".to_string());
                }
                let args = local_shell_call_arguments(action, call_identifier);
                contents.push(GoogleGenAiContent {
                    role: Some("model".to_string()),
                    parts: vec![GoogleGenAiPart {
                        text: None,
                        function_call: Some(GoogleGenAiFunctionCall {
                            name: "local_shell".to_string(),
                            args: Some(args),
                        }),
                        function_response: None,
                    }],
                });
            }
            // Skip other item types that don't map to Google's format
            ResponseItem::Reasoning { .. }
            | ResponseItem::CustomToolCall { .. }
            | ResponseItem::CustomToolCallOutput { .. }
            | ResponseItem::WebSearchCall { .. }
            | ResponseItem::GhostSnapshot { .. }
            | ResponseItem::Other => {
                continue;
            }
        }
    }

    // Convert tools to Google format
    let tools = if !prompt.tools.is_empty() {
        let function_declarations: Vec<GoogleGenAiFunctionDeclaration> = prompt
            .tools
            .iter()
            .filter_map(|tool| match tool {
                ToolSpec::Function(func) => {
                    // Convert JsonSchema to JsonValue and validate
                    let parameters_json = serde_json::to_value(&func.parameters).ok();
                    if let Some(params) = &parameters_json {
                        if !is_valid_json_schema(params) {
                            // Skip invalid schemas
                            None
                        } else {
                            // Process schema to remove unsupported fields like additionalProperties
                            let processed_params = process_json_schema_for_google_genai(params);
                            Some(GoogleGenAiFunctionDeclaration {
                                name: func.name.clone(),
                                description: func.description.clone(),
                                parameters: Some(processed_params),
                            })
                        }
                    } else {
                        None
                    }
                }
                ToolSpec::Freeform(func) => {
                    // For freeform tools, create a basic schema from the format definition
                    // This is a simplified mapping - Google expects JSON Schema format
                    let parameters_json = json!({
                        "type": "object",
                        "description": func.format.definition.clone(),
                    });
                    if is_valid_json_schema(&parameters_json) {
                        let processed_params = process_json_schema_for_google_genai(&parameters_json);
                        Some(GoogleGenAiFunctionDeclaration {
                            name: func.name.clone(),
                            description: func.description.clone(),
                            parameters: Some(processed_params),
                        })
                    } else {
                        None
                    }
                }
                ToolSpec::LocalShell {} => {
                    let parameters_json = local_shell_parameters_schema();
                    if is_valid_json_schema(&parameters_json) {
                        let processed_params = process_json_schema_for_google_genai(&parameters_json);
                        Some(GoogleGenAiFunctionDeclaration {
                            name: "local_shell".to_string(),
                            description: "Execute a shell command on the local machine."
                                .to_string(),
                            parameters: Some(processed_params),
                        })
                    } else {
                        None
                    }
                }
                ToolSpec::WebSearch {} => None, // Google GenAI doesn't support our OpenAI-specific web search helper.
            })
            .collect();

        if !function_declarations.is_empty() {
            Some(vec![GoogleGenAiTool {
                function_declarations,
            }])
        } else {
            None
        }
    } else {
        None
    };

    // Set tool config based on tool_choice
    let tool_config = if tools.is_some() {
        prompt.tool_choice.as_ref().map(|choice| {
            match choice.as_str() {
                "auto" => GoogleGenAiToolConfig {
                    function_calling_config: GoogleGenAiFunctionCallingConfig {
                        mode: "AUTO".to_string(),
                        allowed_function_names: None,
                    },
                },
                "none" => GoogleGenAiToolConfig {
                    function_calling_config: GoogleGenAiFunctionCallingConfig {
                        mode: "NONE".to_string(),
                        allowed_function_names: None,
                    },
                },
                specific => {
                    // Check if specific tool exists in function_declarations
                    let function_names: Vec<String> = tools.as_ref().unwrap()[0]
                        .function_declarations
                        .iter()
                        .map(|fd| fd.name.clone())
                        .collect();
                    if function_names.contains(&specific.to_string()) {
                        GoogleGenAiToolConfig {
                            function_calling_config: GoogleGenAiFunctionCallingConfig {
                                mode: "ANY".to_string(),
                                allowed_function_names: Some(vec![specific.to_string()]),
                            },
                        }
                    } else {
                        // Fallback to AUTO if specific tool not found
                        GoogleGenAiToolConfig {
                            function_calling_config: GoogleGenAiFunctionCallingConfig {
                                mode: "AUTO".to_string(),
                                allowed_function_names: None,
                            },
                        }
                    }
                }
            }
        })
    } else {
        None
    };

    let generation_config = if let Some(config) = &prompt.generation_config {
        Some(GoogleGenAiGenerationConfig {
            temperature: config.temperature,
            top_p: config.top_p,
            max_output_tokens: config.max_tokens.map(|t| t as i32),
            stop_sequences: config.stop_sequences.clone(),
        })
    } else {
        Some(GoogleGenAiGenerationConfig {
            temperature: None,
            top_p: None,
            max_output_tokens: None,
            stop_sequences: None,
        })
    };

    Ok(GoogleGenAiRequest {
        contents,
        system_instruction,
        tools,
        tool_config,
        generation_config,
    })
}

// ============================================================================
// Streaming Implementation
// ============================================================================

/// Main entry point for streaming Google GenAI responses.
///
/// This function:
/// 1. Builds the request payload from the Codex Prompt
/// 2. Sends HTTP POST to Google's streaming endpoint
/// 3. Parses SSE stream into ResponseEvent items
/// 4. Handles retries on transient failures
pub(crate) async fn stream_google_genai(
    prompt: &Prompt,
    model_family: &ModelFamily,
    client: &CodexHttpClient,
    provider: &ModelProviderInfo,
    otel_event_manager: &OtelEventManager,
    _session_source: &SessionSource,
) -> Result<ResponseStream> {
    // Build the request payload
    let request = build_google_genai_request(prompt, model_family)?;
    let payload = serde_json::to_value(&request)?;

    debug!(
        "Google GenAI request payload: {}",
        serde_json::to_string_pretty(&payload).unwrap_or_default()
    );

    let model = &model_family.slug;
    let max_retries = provider.request_max_retries();

    for attempt in 0..=max_retries {
        let url = get_google_genai_streaming_url(provider, model);

        // Create request builder with auth and headers
        let req_builder = match provider
            .create_request_builder_with_url(client, &None, url.clone())
            .await
        {
            Ok(builder) => builder,
            Err(e) => {
                if attempt == max_retries {
                    return Err(e);
                }
                tokio::time::sleep(backoff(attempt)).await;
                continue;
            }
        };

        debug!("POST to Google GenAI: {}", url);

        let res = otel_event_manager
            .log_request(attempt + 1, || {
                req_builder
                    .header(reqwest::header::ACCEPT, "text/event-stream")
                    .json(&payload)
                    .send()
            })
            .await;

        match res {
            Ok(resp) if resp.status().is_success() => {
                debug!("Google GenAI HTTP response successful: status={}", resp.status());
                let (tx_event, rx_event) = mpsc::channel::<Result<ResponseEvent>>(1600);
                let stream = resp.bytes_stream().map_err(|e| {
                    CodexErr::ResponseStreamFailed(ResponseStreamFailed {
                        source: e,
                        request_id: None,
                    })
                });

                tokio::spawn(process_google_genai_sse(
                    stream,
                    tx_event,
                    provider.stream_idle_timeout(),
                    otel_event_manager.clone(),
                ));

                return Ok(ResponseStream { rx_event });
            }
            Ok(res) => {
                let status = res.status();
                let body = res.text().await.unwrap_or_default();
                debug!("Google GenAI HTTP response: status={}, body={}", status, body);

                if !(status == StatusCode::TOO_MANY_REQUESTS || status.is_server_error()) {
                    return Err(CodexErr::UnexpectedStatus(UnexpectedResponseError {
                        status,
                        body,
                        request_id: None,
                    }));
                }

                // Retry on 429 or 5xx
                if attempt < max_retries {
                    tokio::time::sleep(backoff(attempt)).await;
                    continue;
                } else {
                    return Err(CodexErr::UnexpectedStatus(UnexpectedResponseError {
                        status,
                        body,
                        request_id: None,
                    }));
                }
            }
            Err(e) => {
                if attempt < max_retries {
                    tokio::time::sleep(backoff(attempt)).await;
                    continue;
                } else {
                    return Err(CodexErr::ConnectionFailed(ConnectionFailedError {
                        source: e,
                    }));
                }
            }
        }
    }

    // Should never reach here due to the loop logic above
    Err(CodexErr::UnexpectedStatus(UnexpectedResponseError {
        status: StatusCode::INTERNAL_SERVER_ERROR,
        body: "Unexpected: loop ended without return".to_string(),
        request_id: None,
    }))
}

/// Processes the SSE stream from Google GenAI and converts chunks to ResponseEvents.
async fn process_google_genai_sse<S>(
    stream: S,
    tx_event: mpsc::Sender<Result<ResponseEvent>>,
    idle_timeout: Duration,
    otel_event_manager: OtelEventManager,
) where
    S: Stream<Item = Result<Bytes>> + Unpin,
{
    let mut stream = stream.eventsource();

    // Track the streamed assistant message so we can emit deltas and
    // finalize it once the turn is complete.
    let mut assistant_item: Option<ResponseItem> = None;
    let mut next_function_call_index = 0usize;

    loop {
        let start = std::time::Instant::now();
        let response = timeout(idle_timeout, stream.next()).await;
        let duration = start.elapsed();
        otel_event_manager.log_sse_event(&response, duration);

        let sse = match response {
            Ok(Some(Ok(ev))) => {
                debug!("Google GenAI SSE event: event='{}', data='{}'", ev.event, ev.data);
                ev
            }
            Ok(Some(Err(e))) => {
                let _ = tx_event
                    .send(Err(CodexErr::Stream(e.to_string(), None)))
                    .await;
                return;
            }
            Ok(None) => {
                // Stream closed gracefully
                debug!("Google GenAI SSE stream closed gracefully");
                finalize_google_response(&tx_event, &mut assistant_item).await;

                let _ = tx_event
                    .send(Ok(ResponseEvent::Completed {
                        response_id: String::new(),
                        token_usage: None,
                    }))
                    .await;
                return;
            }
            Err(_) => {
                debug!("Google GenAI SSE stream timed out");
                let _ = tx_event
                    .send(Err(CodexErr::Stream(
                        "idle timeout waiting for SSE from Google GenAI".into(),
                        None,
                    )))
                    .await;
                return;
            }
        };

        // Parse the SSE data as JSON
        let chunk: GoogleGenAiStreamChunk = match serde_json::from_str(&sse.data) {
            Ok(v) => v,
            Err(e) => {
                trace!(
                    "Failed to parse Google GenAI SSE chunk: {}, data: {}",
                    e, sse.data
                );
                continue;
            }
        };

        trace!("Google GenAI received chunk: {:?}", chunk);

        // Process candidates
        if let Some(candidate) = chunk.candidates.first() {
            if let Some(content) = &candidate.content {
                for part in &content.parts {
                    // Handle text parts
                    if let Some(text) = &part.text {
                        if !text.is_empty() {
                            if assistant_item.is_none() {
                                let item = ResponseItem::Message {
                                    id: None,
                                    role: "assistant".to_string(),
                                    content: vec![],
                                };
                                let cloned = item.clone();
                                assistant_item = Some(item);
                                let _ = tx_event
                                    .send(Ok(ResponseEvent::OutputItemAdded(cloned)))
                                    .await;
                            }

                            if let Some(ResponseItem::Message { content, .. }) =
                                assistant_item.as_mut()
                            {
                                content.push(ContentItem::OutputText { text: text.clone() });
                            }

                            // Send text delta
                            let _ = tx_event
                                .send(Ok(ResponseEvent::OutputTextDelta(text.clone())))
                                .await;
                        }
                    }

                    // Handle function calls immediately.
                    if let Some(function_call) = &part.function_call {
                        // Validate function call
                        if function_call.name.is_empty() {
                            let _ = tx_event
                                .send(Err(CodexErr::Stream(
                                    "Received function call with empty name from Google GenAI"
                                        .into(),
                                    None,
                                )))
                                .await;
                            continue;
                        }
                        let empty_args = json!({});
                        let args = function_call.args.as_ref().unwrap_or(&empty_args);
                        let arguments =
                            serde_json::to_string(args).unwrap_or_else(|_| "{}".to_string());

                        let item = ResponseItem::FunctionCall {
                            id: None,
                            name: function_call.name.clone(),
                            arguments,
                            call_id: format!("google_call_{}", next_function_call_index),
                        };
                        next_function_call_index += 1;
                        let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
                    }
                }
            }

            // Check if this is the final chunk
            if candidate.finish_reason.is_some() {
                finalize_google_response(&tx_event, &mut assistant_item).await;

                // Send completion event with usage metadata if available
                let token_usage = chunk.usage_metadata.as_ref().map(|usage| {
                    let input_tokens = usage.prompt_token_count.unwrap_or(0) as i64;
                    let output_tokens = usage.candidates_token_count.unwrap_or(0) as i64;
                    let total = usage
                        .total_token_count
                        .map(|t| t as i64)
                        .unwrap_or(input_tokens + output_tokens);
                    codex_protocol::protocol::TokenUsage {
                        input_tokens,
                        cached_input_tokens: 0, // Google doesn't report cached tokens separately
                        output_tokens,
                        reasoning_output_tokens: 0, // Google doesn't report reasoning tokens separately
                        total_tokens: total,
                    }
                });

                let _ = tx_event
                    .send(Ok(ResponseEvent::Completed {
                        response_id: String::new(),
                        token_usage,
                    }))
                    .await;
                return;
            }
        }
    }
}

/// Helper to finalize and send the accumulated response content.
async fn finalize_google_response(
    tx_event: &mpsc::Sender<Result<ResponseEvent>>,
    assistant_item: &mut Option<ResponseItem>,
) {
    if let Some(item) = assistant_item.take() {
        let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
    }
}

fn local_shell_call_arguments(action: &LocalShellAction, call_id: Option<&String>) -> JsonValue {
    match action {
        LocalShellAction::Exec(exec) => {
            let mut obj = serde_json::Map::new();
            obj.insert("command".to_string(), json!(&exec.command));
            if let Some(dir) = &exec.working_directory {
                obj.insert("workdir".to_string(), json!(dir));
            }
            if let Some(timeout) = exec.timeout_ms {
                obj.insert("timeout_ms".to_string(), json!(timeout));
            }
            if let Some(env) = &exec.env {
                obj.insert("env".to_string(), json!(env));
            }
            if let Some(user) = &exec.user {
                obj.insert("user".to_string(), json!(user));
            }
            if let Some(id) = call_id {
                obj.insert("call_id".to_string(), json!(id));
            }
            JsonValue::Object(obj)
        }
    }
}

fn local_shell_parameters_schema() -> JsonValue {
    json!({
        "type": "object",
        "properties": {
            "command": {
                "type": "array",
                "description": "The command to execute",
                "items": { "type": "string" }
            },
            "workdir": {
                "type": "string",
                "description": "The working directory to execute the command in"
            },
            "timeout_ms": {
                "type": "number",
                "description": "The timeout for the command in milliseconds"
            },
            "env": {
                "type": "object",
                "description": "Optional environment variables"
            },
            "user": {
                "type": "string",
                "description": "Optional user to run the command as"
            },
            "with_escalated_permissions": {
                "type": ["boolean", "null"],
                "description": "Whether to request escalated permissions"
            },
            "justification": {
                "type": ["string", "null"],
                "description": "Short justification when escalated permissions are required"
            },
            "call_id": {
                "type": "string",
                "description": "Call identifier for correlating tool outputs"
            }
        },
        "required": ["command"],
        "additionalProperties": false
    })
}

/// Validates if a JSON value is a basic valid JSON Schema for function parameters.
/// Checks for type "object" and presence of properties.
fn is_valid_json_schema(value: &JsonValue) -> bool {
    if let Some(obj) = value.as_object() {
        if let Some(typ) = obj.get("type") {
            if typ == "object" {
                // Basic check: has type "object"
                return true;
            }
        }
    }
    false
}

// ============================================================================
// Tests
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;
    use crate::client_common::tools::FreeformTool;
    use crate::client_common::tools::FreeformToolFormat;
    use crate::client_common::tools::ResponsesApiTool;
    use crate::config::types::ReasoningSummaryFormat;
    use crate::tools::spec::ConfigShellToolType;
    use crate::tools::spec::JsonSchema;
    use codex_app_server_protocol::AuthMode;
    use codex_protocol::ConversationId;
    use codex_protocol::models::ContentItem;
    use codex_protocol::models::FunctionCallOutputPayload;
    use codex_protocol::models::LocalShellAction;
    use codex_protocol::models::LocalShellExecAction;
    use codex_protocol::models::LocalShellStatus;
    use futures::stream;
    use std::collections::BTreeMap;
    use tokio::time::Duration;

    fn create_test_model_family() -> ModelFamily {
        ModelFamily {
            family: "gemini".to_string(),
            slug: "gemini-2.0-flash".to_string(),
            base_instructions: "You are a helpful assistant.".into(),
            default_reasoning_effort: None,
            supports_reasoning_summaries: false,
            reasoning_summary_format: ReasoningSummaryFormat::None,
            support_verbosity: false,
            default_verbosity: None,
            effective_context_window_percent: 100,
            needs_special_apply_patch_instructions: false,
            supports_parallel_tool_calls: false,
            apply_patch_tool_type: None,
            experimental_supported_tools: Vec::new(),
            shell_type: ConfigShellToolType::Default,
        }
    }

    fn create_test_prompt() -> Prompt {
        Prompt {
            input: vec![ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![ContentItem::InputText {
                    text: "Hello, world!".to_string(),
                }],
            }],
            tools: vec![],
            parallel_tool_calls: false,
            tool_choice: Some("auto".to_string()),
            base_instructions_override: None,
            output_schema: None,
            generation_config: None,
        }
    }

    #[test]
    fn test_simple_text_message_mapping() {
        let model_family = create_test_model_family();
        let prompt = create_test_prompt();

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        // Check system instruction
        assert!(request.system_instruction.is_some());
        let system_content = request.system_instruction.unwrap();
        assert_eq!(system_content.role.as_ref().unwrap(), "user");
        assert_eq!(system_content.parts.len(), 1);
        assert_eq!(system_content.parts[0].text.as_ref().unwrap(), "You are a helpful assistant.");

        // Check contents
        assert_eq!(request.contents.len(), 1);
        let content = &request.contents[0];
        assert_eq!(content.role.as_ref().unwrap(), "user");
        assert_eq!(content.parts.len(), 1);
        assert_eq!(content.parts[0].text.as_ref().unwrap(), "Hello, world!");
    }

    #[test]
    fn test_system_instructions_at_top_level() {
        let model_family = create_test_model_family();
        let prompt = create_test_prompt();

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        // Ensure system instructions are correctly placed at top level
        assert!(request.system_instruction.is_some());
        let system_content = request.system_instruction.unwrap();
        assert_eq!(system_content.role.as_ref().unwrap(), "user");
        assert_eq!(system_content.parts.len(), 1);
        assert_eq!(system_content.parts[0].text.as_ref().unwrap(), "You are a helpful assistant.");
    }

    #[test]
    fn test_no_system_instructions_when_empty() {
        let mut model_family = create_test_model_family();
        model_family.base_instructions = "".to_string();
        let prompt = create_test_prompt();

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        // Ensure no system instructions when empty
        assert!(request.system_instruction.is_none());
    }

    #[test]
    fn test_assistant_role_mapping() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.input = vec![
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![ContentItem::InputText {
                    text: "Hello".to_string(),
                }],
            },
            ResponseItem::Message {
                id: None,
                role: "assistant".to_string(),
                content: vec![ContentItem::OutputText {
                    text: "Hi there!".to_string(),
                }],
            },
        ];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert_eq!(request.contents.len(), 2);
        assert_eq!(request.contents[0].role.as_ref().unwrap(), "user");
        assert_eq!(request.contents[1].role.as_ref().unwrap(), "model"); // Google uses "model" not "assistant"
        assert_eq!(
            request.contents[1].parts[0].text.as_ref().unwrap(),
            "Hi there!"
        );
    }

    #[test]
    fn test_function_call_mapping() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.input = vec![ResponseItem::FunctionCall {
            id: None,
            name: "get_weather".to_string(),
            arguments: r#"{"location":"Paris"}"#.to_string(),
            call_id: "call_123".to_string(),
        }];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert_eq!(request.contents.len(), 1);
        let content = &request.contents[0];
        assert_eq!(content.role, "model");
        assert!(content.parts[0].function_call.is_some());

        let func_call = content.parts[0].function_call.as_ref().unwrap();
        assert_eq!(func_call.name, "get_weather");
        assert!(func_call.args.is_some());
    }

    #[test]
    fn test_function_response_mapping() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.input = vec![
            ResponseItem::FunctionCall {
                id: None,
                name: "get_weather".to_string(),
                arguments: "{}".to_string(),
                call_id: "call_123".to_string(),
            },
            ResponseItem::FunctionCallOutput {
                call_id: "call_123".to_string(),
                output: FunctionCallOutputPayload {
                    success: Some(true),
                    content: "The weather is sunny".to_string(),
                    content_items: None,
                },
            },
        ];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert_eq!(request.contents.len(), 2);
        let content = &request.contents[1];
        assert_eq!(content.role, "user"); // Function responses come from user
        assert!(content.parts[0].function_response.is_some());

        let func_resp = content.parts[0].function_response.as_ref().unwrap();
        assert_eq!(func_resp.name, "get_weather");
    }

    #[test]
    fn test_multi_turn_conversation_mapping() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.input = vec![
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![ContentItem::InputText {
                    text: "What's the weather?".to_string(),
                }],
            },
            ResponseItem::Message {
                id: None,
                role: "assistant".to_string(),
                content: vec![ContentItem::OutputText {
                    text: "I'll check.".to_string(),
                }],
            },
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![ContentItem::InputText {
                    text: "Thanks!".to_string(),
                }],
            },
        ];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert_eq!(request.contents.len(), 3);
        assert_eq!(request.contents[0].role.as_ref().unwrap(), "user");
        assert_eq!(request.contents[1].role.as_ref().unwrap(), "model");
        assert_eq!(request.contents[2].role.as_ref().unwrap(), "user");
    }

    #[test]
    fn test_tool_spec_conversion_function() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        let mut properties = BTreeMap::new();
        properties.insert(
            "location".to_string(),
            JsonSchema::String {
                description: Some("Location name".to_string()),
            },
        );

        prompt.tools = vec![ToolSpec::Function(ResponsesApiTool {
            name: "get_weather".to_string(),
            description: "Get the weather for a location".to_string(),
            strict: false,
            parameters: JsonSchema::Object {
                properties,
                required: None,
                additional_properties: None,
            },
        })];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert!(request.tools.is_some());
        let tools = request.tools.unwrap();
        assert_eq!(tools.len(), 1);
        assert_eq!(tools[0].function_declarations.len(), 1);

        let func_decl = &tools[0].function_declarations[0];
        assert_eq!(func_decl.name, "get_weather");
        assert_eq!(func_decl.description, "Get the weather for a location");
        assert!(func_decl.parameters.is_some());
    }

    #[test]
    fn test_tool_spec_conversion_freeform() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.tools = vec![ToolSpec::Freeform(FreeformTool {
            name: "shell".to_string(),
            description: "Execute shell command".to_string(),
            format: FreeformToolFormat {
                r#type: "custom".to_string(),
                syntax: "bash".to_string(),
                definition: "Executes a shell command and returns the output".to_string(),
            },
        })];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert!(request.tools.is_some());
        let tools = request.tools.unwrap();
        assert_eq!(tools.len(), 1);
        assert_eq!(tools[0].function_declarations.len(), 1);

        let func_decl = &tools[0].function_declarations[0];
        assert_eq!(func_decl.name, "shell");
        assert_eq!(func_decl.description, "Execute shell command");
    }

    #[test]
    fn test_tool_spec_conversion_local_shell() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.tools = vec![ToolSpec::LocalShell {}];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert!(request.tools.is_some());
        let tools = request.tools.unwrap();
        assert_eq!(tools.len(), 1);
        assert_eq!(tools[0].function_declarations.len(), 1);

        let func_decl = &tools[0].function_declarations[0];
        assert_eq!(func_decl.name, "local_shell");
        assert!(func_decl.parameters.is_some());
    }

    #[test]
    fn test_empty_message_skipped() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.input = vec![
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![ContentItem::InputText {
                    text: "".to_string(), // Empty text
                }],
            },
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![ContentItem::InputText {
                    text: "Real message".to_string(),
                }],
            },
        ];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        // Empty messages should be skipped
        assert_eq!(request.contents.len(), 1);
        assert_eq!(
            request.contents[0].parts[0].text.as_ref().unwrap(),
            "Real message"
        );
    }

    #[test]
    fn test_url_construction() {
        use crate::model_provider_info::create_google_genai_provider;

        let provider = create_google_genai_provider();
        let url = get_google_genai_streaming_url(&provider, "gemini-2.0-flash");

        assert!(url.contains("generativelanguage.googleapis.com"));
        assert!(url.contains("/v1beta/models/gemini-2.0-flash:streamGenerateContent"));
        assert!(!url.contains("{model}")); // Placeholder should be replaced
    }

    #[test]
    fn test_parse_google_genai_text_chunk() {
        let chunk_json = r#"{
            "candidates": [{
                "content": {
                    "role": "model",
                    "parts": [{"text": "Hello"}]
                }
            }]
        }"#;

        let chunk: GoogleGenAiStreamChunk = serde_json::from_str(chunk_json).unwrap();

        assert_eq!(chunk.candidates.len(), 1);
        let candidate = &chunk.candidates[0];
        assert!(candidate.content.is_some());

        let content = candidate.content.as_ref().unwrap();
        assert_eq!(content.role, "model");
        assert_eq!(content.parts.len(), 1);
        assert_eq!(content.parts[0].text.as_ref().unwrap(), "Hello");
    }

    #[test]
    fn test_parse_google_genai_function_call_chunk() {
        let chunk_json = r#"{
            "candidates": [{
                "content": {
                    "role": "model",
                    "parts": [{
                        "functionCall": {
                            "name": "get_weather",
                            "args": {"location": "Paris"}
                        }
                    }]
                }
            }]
        }"#;

        let chunk: GoogleGenAiStreamChunk = serde_json::from_str(chunk_json).unwrap();

        let candidate = &chunk.candidates[0];
        let content = candidate.content.as_ref().unwrap();
        let part = &content.parts[0];

        assert!(part.function_call.is_some());
        let func_call = part.function_call.as_ref().unwrap();
        assert_eq!(func_call.name, "get_weather");
        assert!(func_call.args.is_some());
    }

    #[test]
    fn test_parse_google_genai_completion_chunk() {
        let chunk_json = r#"{
            "candidates": [{
                "content": {
                    "role": "model",
                    "parts": [{"text": "Done"}]
                },
                "finishReason": "STOP"
            }],
            "usageMetadata": {
                "promptTokenCount": 10,
                "candidatesTokenCount": 20,
                "totalTokenCount": 30
            }
        }"#;

        let chunk: GoogleGenAiStreamChunk = serde_json::from_str(chunk_json).unwrap();

        let candidate = &chunk.candidates[0];
        assert_eq!(candidate.finish_reason.as_ref().unwrap(), "STOP");

        assert!(chunk.usage_metadata.is_some());
        let usage = chunk.usage_metadata.as_ref().unwrap();
        assert_eq!(usage.prompt_token_count.unwrap(), 10);
        assert_eq!(usage.candidates_token_count.unwrap(), 20);
        assert_eq!(usage.total_token_count.unwrap(), 30);
    }

    #[test]
    fn test_parse_malformed_chunk() {
        let chunk_json = r#"{"invalid": "json"}"#;

        let result: std::result::Result<GoogleGenAiStreamChunk, _> =
            serde_json::from_str(chunk_json);

        // Should still parse but have empty candidates
        assert!(result.is_ok());
        let chunk = result.unwrap();
        assert_eq!(chunk.candidates.len(), 0);
    }

    #[test]
    fn test_local_shell_call_mapping() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.input = vec![ResponseItem::LocalShellCall {
            id: Some("shell_call".to_string()),
            call_id: Some("call_123".to_string()),
            status: LocalShellStatus::Completed,
            action: LocalShellAction::Exec(LocalShellExecAction {
                command: vec!["ls".to_string(), "-a".to_string()],
                timeout_ms: Some(5000),
                working_directory: Some("/tmp".to_string()),
                env: None,
                user: None,
            }),
        }];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert_eq!(request.contents.len(), 1);
        let content = &request.contents[0];
        assert_eq!(content.role, "model");
        let part = &content.parts[0];
        let function_call = part.function_call.as_ref().unwrap();
        assert_eq!(function_call.name, "local_shell");
        let args = function_call.args.as_ref().unwrap();
        assert_eq!(args["command"], json!(["ls", "-a"]));
        assert_eq!(args["workdir"], json!("/tmp"));
        assert_eq!(args["timeout_ms"], json!(5000));
        assert_eq!(args["call_id"], json!("call_123"));
    }

    #[test]
    fn test_generation_config_mapping() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.generation_config = Some(crate::client_common::GenerationConfig {
            temperature: Some(0.8),
            top_p: Some(0.9),
            max_tokens: Some(200),
            stop_sequences: Some(vec!["stop".to_string(), "end".to_string()]),
            presence_penalty: Some(0.1), // Unsupported, should be ignored
            frequency_penalty: Some(0.2), // Unsupported, should be ignored
            seed: Some(123),             // Unsupported, should be ignored
        });

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        let config = request.generation_config.unwrap();
        assert_eq!(config.temperature, Some(0.8));
        assert_eq!(config.top_p, Some(0.9));
        assert_eq!(config.max_output_tokens, Some(200));
        assert_eq!(
            config.stop_sequences,
            Some(vec!["stop".to_string(), "end".to_string()])
        );
        assert!(request.system_instruction.is_none()); // No system instruction in test
    }

    #[test]
    fn test_json_schema_processing_removes_additional_properties() {
        let schema_with_additional_properties = json!({
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "age": {"type": "number"}
            },
            "additionalProperties": false
        });

        let processed = process_json_schema_for_google_genai(&schema_with_additional_properties);

        // additionalProperties should be removed
        assert!(processed.get("additionalProperties").is_none());

        // Other fields should remain
        assert_eq!(processed.get("type").unwrap(), "object");
        assert!(processed.get("properties").is_some());
    }

    #[test]
    fn test_generation_config_none_values() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.generation_config = Some(crate::client_common::GenerationConfig {
            temperature: None,
            top_p: None,
            max_tokens: None,
            stop_sequences: None,
            presence_penalty: None,
            frequency_penalty: None,
            seed: None,
        });

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        let config = request.generation_config.unwrap();
        assert_eq!(config.temperature, None);
        assert_eq!(config.top_p, None);
        assert_eq!(config.max_output_tokens, None);
        assert_eq!(config.stop_sequences, None);
    }

    #[test]
    fn test_tool_choice_auto() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.tool_choice = Some("auto".to_string());
        prompt.tools = vec![ToolSpec::LocalShell {}];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert!(request.tool_config.is_some());
        let tool_config = request.tool_config.unwrap();
        assert_eq!(tool_config.function_calling_config.mode, "AUTO");
        assert!(
            tool_config
                .function_calling_config
                .allowed_function_names
                .is_none()
        );
    }

    #[test]
    fn test_tool_choice_none() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.tool_choice = Some("none".to_string());
        prompt.tools = vec![ToolSpec::LocalShell {}];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert!(request.tool_config.is_some());
        let tool_config = request.tool_config.unwrap();
        assert_eq!(tool_config.function_calling_config.mode, "NONE");
        assert!(
            tool_config
                .function_calling_config
                .allowed_function_names
                .is_none()
        );
    }

    #[test]
    fn test_tool_choice_specific() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.tool_choice = Some("local_shell".to_string());
        prompt.tools = vec![ToolSpec::LocalShell {}];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        assert!(request.tool_config.is_some());
        let tool_config = request.tool_config.unwrap();
        assert_eq!(tool_config.function_calling_config.mode, "ANY");
        assert_eq!(
            tool_config.function_calling_config.allowed_function_names,
            Some(vec!["local_shell".to_string()])
        );
    }

    #[test]
    fn test_tool_choice_specific_not_found() {
        let model_family = create_test_model_family();
        let mut prompt = create_test_prompt();
        prompt.tool_choice = Some("nonexistent".to_string());
        prompt.tools = vec![ToolSpec::LocalShell {}];

        let request = build_google_genai_request(&prompt, &model_family).unwrap();

        // Should fallback to AUTO
        assert!(request.tool_config.is_some());
        let tool_config = request.tool_config.unwrap();
        assert_eq!(tool_config.function_calling_config.mode, "AUTO");
    }

    #[tokio::test]
    async fn test_stream_emits_assistant_item_before_deltas() {
        let (tx, mut rx) = tokio::sync::mpsc::channel(16);
        let text_chunk =
            r#"{"candidates":[{"content":{"role":"model","parts":[{"text":"Hello"}]}}]}"#;
        let done_chunk = r#"{"candidates":[{"finishReason":"STOP"}]}"#;

        process_google_genai_sse(
            stream::iter(vec![
                Ok(Bytes::from(format!("data: {text_chunk}\n\n"))),
                Ok(Bytes::from(format!("data: {done_chunk}\n\n"))),
            ]),
            tx,
            Duration::from_secs(1),
            test_otel_event_manager(),
        )
        .await;

        match rx.recv().await.unwrap().unwrap() {
            ResponseEvent::OutputItemAdded(ResponseItem::Message { role, content, .. }) => {
                assert_eq!(role, "assistant");
                assert!(content.is_empty());
            }
            other => panic!("expected OutputItemAdded for assistant, got {other:?}"),
        }

        match rx.recv().await.unwrap().unwrap() {
            ResponseEvent::OutputTextDelta(delta) => assert_eq!(delta, "Hello"),
            other => panic!("expected OutputTextDelta, got {other:?}"),
        }

        match rx.recv().await.unwrap().unwrap() {
            ResponseEvent::OutputItemDone(ResponseItem::Message { role, content, .. }) => {
                assert_eq!(role, "assistant");
                assert_eq!(content.len(), 1);
            }
            other => panic!("expected OutputItemDone for assistant, got {other:?}"),
        }

        match rx.recv().await.unwrap().unwrap() {
            ResponseEvent::Completed { .. } => {}
            other => panic!("expected Completed, got {other:?}"),
        }
    }

    #[tokio::test]
    async fn test_function_call_emitted_immediately() {
        let (tx, mut rx) = tokio::sync::mpsc::channel(16);
        let call_chunk = r#"{"candidates":[{"content":{"role":"model","parts":[{"functionCall":{"name":"lookup","args":{"city":"Paris"}}}]}}]}"#;
        let done_chunk = r#"{"candidates":[{"finishReason":"STOP"}]}"#;

        process_google_genai_sse(
            stream::iter(vec![
                Ok(Bytes::from(format!("data: {call_chunk}\n\n"))),
                Ok(Bytes::from(format!("data: {done_chunk}\n\n"))),
            ]),
            tx,
            Duration::from_secs(1),
            test_otel_event_manager(),
        )
        .await;

        match rx.recv().await.unwrap().unwrap() {
            ResponseEvent::OutputItemDone(ResponseItem::FunctionCall {
                name, arguments, ..
            }) => {
                assert_eq!(name, "lookup");
                assert_eq!(arguments, r#"{"city":"Paris"}"#);
            }
            other => panic!("expected immediate FunctionCall, got {other:?}"),
        }

        match rx.recv().await.unwrap().unwrap() {
            ResponseEvent::Completed { .. } => {}
            other => panic!("expected Completed, got {other:?}"),
        }
    }

    fn test_otel_event_manager() -> OtelEventManager {
        OtelEventManager::new(
            ConversationId::new(),
            "test-model",
            "test-slug",
            None,
            None,
            None::<AuthMode>,
            false,
            "test".to_string(),
        )
    }
}
